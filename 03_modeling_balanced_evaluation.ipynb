{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b83c49-676d-44e2-8c15-e73a60ab96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19029\n",
      "Class counts (3-class):\n",
      "WQ_3C\n",
      "Poor          10026\n",
      "Unsuitable     6608\n",
      "Good           2395\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: {'Poor': 0.632655096748454, 'Unsuitable': 0.9598970944309927, 'Good': 2.648434237995825}\n",
      "\n",
      "============================================================\n",
      "Evaluating: LogReg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogReg] Fold 1: macro-F1=0.9691 | bal-acc=0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogReg] Fold 2: macro-F1=0.9695 | bal-acc=0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogReg] Fold 3: macro-F1=0.9708 | bal-acc=0.9826\n",
      "[LogReg] Fold 4: macro-F1=0.9606 | bal-acc=0.9794\n",
      "[LogReg] Fold 5: macro-F1=0.9633 | bal-acc=0.9795\n",
      "\n",
      "LogReg | macro-F1: 0.9667 ± 0.0040 | bal-acc: 0.9819 ± 0.0021\n",
      "\n",
      "Confusion matrix (summed over folds) in order [Good, Poor, Unsuitable]:\n",
      "            Good  Poor  Unsuitable\n",
      "Good        2387     8           0\n",
      "Poor         307  9596         123\n",
      "Unsuitable     0    53        6555\n",
      "\n",
      "============================================================\n",
      "Evaluating: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got ['Good' 'Poor' 'Unsuitable']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     f1m, bam \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m: kind, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbal_acc_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: bam})\n\u001b[1;32m    143\u001b[0m res_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbal_acc_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(name, model, X, y)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# per-sample weights for training folds\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     sw \u001b[38;5;241m=\u001b[39m per_sample_weights(y_tr, class_weights)\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/xgboost/core.py:705\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    704\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1641\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1638\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1640\u001b[0m ):\n\u001b[0;32m-> 1641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m     )\n\u001b[1;32m   1646\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got ['Good' 'Poor' 'Unsuitable']"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Models & utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# XGBoost (installed on your Mac)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load data and select features\n",
    "# ---------------------------\n",
    "df = pd.read_csv(\"waterDataset.csv\")\n",
    "\n",
    "# Keep the six core features (you trained your latest model on these)\n",
    "FEATURES = [\"EC\", \"TDS\", \"Na\", \"TH\", \"Cl\", \"pH\"]\n",
    "assert all(c in df.columns for c in FEATURES), f\"Missing expected columns. Found: {df.columns}\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Map 5 classes -> 3 classes (as requested)\n",
    "#    Good bucket:      [\"Excellent\",\"Good\"]\n",
    "#    Poor bucket:      [\"Very Poor yet Drinkable\",\"Poor\"]\n",
    "#    Unsuitable bucket:[\"Unsuitable for Drinking\"]\n",
    "# ---------------------------\n",
    "def map_to_3_classes(label: str) -> str:\n",
    "    if label in [\"Excellent\", \"Good\"]:\n",
    "        return \"Good\"\n",
    "    elif label in [\"Poor\", \"Very Poor yet Drinkable\"]:\n",
    "        return \"Poor\"\n",
    "    elif label == \"Unsuitable for Drinking\":\n",
    "        return \"Unsuitable\"\n",
    "    else:\n",
    "        return None  # unexpected\n",
    "\n",
    "df[\"WQ_3C\"] = df[\"Water Quality Classification\"].map(map_to_3_classes)\n",
    "df = df.dropna(subset=[\"WQ_3C\"])\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[\"WQ_3C\"].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Class counts (3-class):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Compute class weights (for imbalance)\n",
    "# ---------------------------\n",
    "class_counts = y.value_counts()\n",
    "total = len(y)\n",
    "class_weights = {cls: total/(len(class_counts)*cnt) for cls, cnt in class_counts.items()}\n",
    "print(\"\\nClass weights:\", class_weights)\n",
    "\n",
    "def per_sample_weights(y_series, cw):\n",
    "    return y_series.map(cw).values\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Define candidates\n",
    "#    A) Logistic Regression (multinomial) with scaler + class_weight='balanced'\n",
    "#    B) XGBoost with sample_weight per fold\n",
    "# ---------------------------\n",
    "logreg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=3000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    n_estimators=600,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.06,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "candidates = [\n",
    "    (\"LogReg\", logreg_pipe, \"linear\"),\n",
    "    (\"XGBoost\", xgb, \"tree\"),\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Manual CV loop (so we can pass sample_weight cleanly)\n",
    "# ---------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_model(name, model, X, y):\n",
    "    f1s, bals = [], []\n",
    "    all_cm = np.zeros((3,3), dtype=int)  # in order [Good, Poor, Unsuitable]\n",
    "    order = [\"Good\", \"Poor\", \"Unsuitable\"]\n",
    "\n",
    "    for fold, (tr, te) in enumerate(cv.split(X, y), start=1):\n",
    "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
    "        y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        if name == \"XGBoost\":\n",
    "            # per-sample weights for training folds\n",
    "            sw = per_sample_weights(y_tr, class_weights)\n",
    "            model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = model.predict(X_te)\n",
    "\n",
    "        f1 = f1_score(y_te, y_pred, average=\"macro\")\n",
    "        ba = balanced_accuracy_score(y_te, y_pred)\n",
    "\n",
    "        f1s.append(f1)\n",
    "        bals.append(ba)\n",
    "\n",
    "        # accumulate confusion matrix in a fixed label order\n",
    "        cm = confusion_matrix(y_te, y_pred, labels=order)\n",
    "        all_cm += cm\n",
    "\n",
    "        print(f\"[{name}] Fold {fold}: macro-F1={f1:.4f} | bal-acc={ba:.4f}\")\n",
    "\n",
    "    print(f\"\\n{name} | macro-F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f} | \"\n",
    "          f\"bal-acc: {np.mean(bals):.4f} ± {np.std(bals):.4f}\")\n",
    "    print(\"\\nConfusion matrix (summed over folds) in order [Good, Poor, Unsuitable]:\")\n",
    "    print(pd.DataFrame(all_cm, index=order, columns=order))\n",
    "    return np.mean(f1s), np.mean(bals)\n",
    "\n",
    "results = []\n",
    "for name, model, kind in candidates:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    f1m, bam = evaluate_model(name, model, X, y)\n",
    "    results.append({\"model\": name, \"kind\": kind, \"f1_macro_mean\": f1m, \"bal_acc_mean\": bam})\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(by=[\"f1_macro_mean\",\"bal_acc_mean\"], ascending=False)\n",
    "print(\"\\n\\nCV ranking:\")\n",
    "print(res_df)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Fit BEST model on FULL data & save\n",
    "# ---------------------------\n",
    "best_name = res_df.iloc[0][\"model\"]\n",
    "print(f\"\\nBest model selected: {best_name}\")\n",
    "\n",
    "if best_name == \"XGBoost\":\n",
    "    sw_full = per_sample_weights(y, class_weights)\n",
    "    xgb.fit(X, y, sample_weight=sw_full)\n",
    "    best_model = xgb\n",
    "else:\n",
    "    logreg_pipe.fit(X, y)\n",
    "    best_model = logreg_pipe\n",
    "\n",
    "out_file = \"water_quality_rf_model_3class.pkl\" if best_name==\"XGBoost\" else \"water_quality_logreg_3class.pkl\"\n",
    "joblib.dump(best_model, out_file)\n",
    "print(f\"✅ Saved best 3-class model as: {out_file}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Quick sanity check on FULL fit\n",
    "# ---------------------------\n",
    "y_pred_full = best_model.predict(X)\n",
    "print(\"\\nFull-data classification report (sanity check, not a test metric):\")\n",
    "print(classification_report(y, y_pred_full, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b577015d-c926-49d7-8253-bf6518e4cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19029\n",
      "Class counts (3-class):\n",
      "WQ_3C\n",
      "Poor          10026\n",
      "Unsuitable     6608\n",
      "Good           2395\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: {'Poor': 0.632655096748454, 'Unsuitable': 0.9598970944309927, 'Good': 2.648434237995825}\n",
      "\n",
      "============================================================\n",
      "Evaluating: LogReg\n",
      "[LogReg] Fold 1: macro-F1=0.9691 | bal-acc=0.9840\n",
      "[LogReg] Fold 2: macro-F1=0.9695 | bal-acc=0.9841\n",
      "[LogReg] Fold 3: macro-F1=0.9708 | bal-acc=0.9826\n",
      "[LogReg] Fold 4: macro-F1=0.9606 | bal-acc=0.9794\n",
      "[LogReg] Fold 5: macro-F1=0.9633 | bal-acc=0.9795\n",
      "\n",
      "LogReg | macro-F1: 0.9667 ± 0.0040 | bal-acc: 0.9819 ± 0.0021\n",
      "\n",
      "Confusion matrix (summed) [Good, Poor, Unsuitable]:\n",
      "            Good  Poor  Unsuitable\n",
      "Good        2387     8           0\n",
      "Poor         307  9596         123\n",
      "Unsuitable     0    53        6555\n",
      "\n",
      "============================================================\n",
      "Evaluating: XGBoost\n",
      "[XGB ] Fold 1: macro-F1=0.9851 | bal-acc=0.9855\n",
      "[XGB ] Fold 2: macro-F1=0.9839 | bal-acc=0.9874\n",
      "[XGB ] Fold 3: macro-F1=0.9847 | bal-acc=0.9842\n",
      "[XGB ] Fold 4: macro-F1=0.9835 | bal-acc=0.9847\n",
      "[XGB ] Fold 5: macro-F1=0.9849 | bal-acc=0.9868\n",
      "\n",
      "XGBoost | macro-F1: 0.9844 ± 0.0006 | bal-acc: 0.9857 ± 0.0012\n",
      "\n",
      "Confusion matrix (summed) [Good, Poor, Unsuitable]:\n",
      "            Good  Poor  Unsuitable\n",
      "Good        2354    41           0\n",
      "Poor          63  9874          89\n",
      "Unsuitable     0    70        6538\n",
      "\n",
      "\n",
      "CV ranking:\n",
      "     model  f1_macro_mean  bal_acc_mean    kind\n",
      "1  XGBoost       0.984411      0.985709    tree\n",
      "0   LogReg       0.966663      0.981917  linear\n",
      "\n",
      "Best model selected: XGBoost\n",
      "✅ Saved best 3-class model as: water_quality_xgb_3class.pkl\n",
      "\n",
      "Full-data classification report (sanity check):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good     1.0000    1.0000    1.0000      2395\n",
      "        Poor     1.0000    1.0000    1.0000     10026\n",
      "  Unsuitable     1.0000    1.0000    1.0000      6608\n",
      "\n",
      "    accuracy                         1.0000     19029\n",
      "   macro avg     1.0000    1.0000    1.0000     19029\n",
      "weighted avg     1.0000    1.0000    1.0000     19029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 03_model_selection.ipynb  (single fixed cell)\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load data & select features\n",
    "# ---------------------------\n",
    "df = pd.read_csv(\"waterDataset.csv\")\n",
    "FEATURES = [\"EC\", \"TDS\", \"Na\", \"TH\", \"Cl\", \"pH\"]\n",
    "assert all(c in df.columns for c in FEATURES), f\"Missing columns. Found: {df.columns}\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Collapse 5 classes -> 3 classes (as discussed)\n",
    "# ---------------------------\n",
    "def map_to_3_classes(label: str) -> str:\n",
    "    if label in [\"Excellent\", \"Good\"]:\n",
    "        return \"Good\"\n",
    "    elif label in [\"Poor\", \"Very Poor yet Drinkable\"]:\n",
    "        return \"Poor\"\n",
    "    elif label == \"Unsuitable for Drinking\":\n",
    "        return \"Unsuitable\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"WQ_3C\"] = df[\"Water Quality Classification\"].map(map_to_3_classes)\n",
    "df = df.dropna(subset=[\"WQ_3C\"])\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[\"WQ_3C\"].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Class counts (3-class):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Class weights for imbalance\n",
    "# ---------------------------\n",
    "class_counts = y.value_counts()\n",
    "K = len(class_counts)\n",
    "N = len(y)\n",
    "class_weights = {cls: N/(K*cnt) for cls, cnt in class_counts.items()}\n",
    "print(\"\\nClass weights:\", class_weights)\n",
    "\n",
    "def per_sample_weights(y_series, cw):\n",
    "    return y_series.map(cw).values\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Models\n",
    "# ---------------------------\n",
    "logreg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=3000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# For XGB we will encode y -> integers [0,1,2]\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)               # ['Good','Poor','Unsuitable'] -> e.g. [0,1,2]\n",
    "label_order = list(le.classes_)           # keep string order for reporting\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    n_estimators=600,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.06,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "candidates = [\n",
    "    (\"LogReg\", logreg_pipe, \"linear\"),\n",
    "    (\"XGBoost\", xgb, \"tree\"),\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Manual CV (so we can pass weights to XGB)\n",
    "# ---------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_logreg(model, X, y):\n",
    "    f1s, bals = [], []\n",
    "    order = [\"Good\", \"Poor\", \"Unsuitable\"]\n",
    "    all_cm = np.zeros((3,3), dtype=int)\n",
    "\n",
    "    for fold, (tr, te) in enumerate(cv.split(X, y), start=1):\n",
    "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
    "        y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_te)\n",
    "\n",
    "        f1 = f1_score(y_te, y_pred, average=\"macro\")\n",
    "        ba = balanced_accuracy_score(y_te, y_pred)\n",
    "        f1s.append(f1); bals.append(ba)\n",
    "\n",
    "        cm = confusion_matrix(y_te, y_pred, labels=order)\n",
    "        all_cm += cm\n",
    "        print(f\"[LogReg] Fold {fold}: macro-F1={f1:.4f} | bal-acc={ba:.4f}\")\n",
    "\n",
    "    print(f\"\\nLogReg | macro-F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f} | \"\n",
    "          f\"bal-acc: {np.mean(bals):.4f} ± {np.std(bals):.4f}\")\n",
    "    print(\"\\nConfusion matrix (summed) [Good, Poor, Unsuitable]:\")\n",
    "    print(pd.DataFrame(all_cm, index=order, columns=order))\n",
    "    return np.mean(f1s), np.mean(bals)\n",
    "\n",
    "def evaluate_xgb(model, X, y_str, y_int):\n",
    "    f1s, bals = [], []\n",
    "    order = label_order  # ['Good','Poor','Unsuitable']\n",
    "    all_cm = np.zeros((3,3), dtype=int)\n",
    "\n",
    "    for fold, (tr, te) in enumerate(cv.split(X, y_int), start=1):\n",
    "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
    "        y_tr_int, y_te_int = y_int[tr], y_int[te]\n",
    "        y_tr_str, y_te_str = y_str.iloc[tr], y_str.iloc[te]\n",
    "\n",
    "        sw = per_sample_weights(pd.Series(y_tr_str), class_weights)\n",
    "        model.fit(X_tr, y_tr_int, sample_weight=sw)\n",
    "\n",
    "        y_pred_int = model.predict(X_te)\n",
    "        y_pred_str = le.inverse_transform(y_pred_int)   # back to strings\n",
    "\n",
    "        f1 = f1_score(y_str.iloc[te], y_pred_str, average=\"macro\")\n",
    "        ba = balanced_accuracy_score(y_str.iloc[te], y_pred_str)\n",
    "        f1s.append(f1); bals.append(ba)\n",
    "\n",
    "        cm = confusion_matrix(y_str.iloc[te], y_pred_str, labels=order)\n",
    "        all_cm += cm\n",
    "        print(f\"[XGB ] Fold {fold}: macro-F1={f1:.4f} | bal-acc={ba:.4f}\")\n",
    "\n",
    "    print(f\"\\nXGBoost | macro-F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f} | \"\n",
    "          f\"bal-acc: {np.mean(bals):.4f} ± {np.std(bals):.4f}\")\n",
    "    print(\"\\nConfusion matrix (summed) [Good, Poor, Unsuitable]:\")\n",
    "    print(pd.DataFrame(all_cm, index=order, columns=order))\n",
    "    return np.mean(f1s), np.mean(bals)\n",
    "\n",
    "# Run both\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nEvaluating: LogReg\")\n",
    "log_f1, log_ba = evaluate_logreg(logreg_pipe, X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nEvaluating: XGBoost\")\n",
    "xgb_f1, xgb_ba = evaluate_xgb(xgb, X, y, y_int)\n",
    "\n",
    "res_df = pd.DataFrame([\n",
    "    {\"model\": \"LogReg\",  \"f1_macro_mean\": log_f1, \"bal_acc_mean\": log_ba, \"kind\": \"linear\"},\n",
    "    {\"model\": \"XGBoost\", \"f1_macro_mean\": xgb_f1, \"bal_acc_mean\": xgb_ba, \"kind\": \"tree\"},\n",
    "]).sort_values(by=[\"f1_macro_mean\",\"bal_acc_mean\"], ascending=False)\n",
    "print(\"\\n\\nCV ranking:\")\n",
    "print(res_df)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Fit BEST on FULL data & save\n",
    "# ---------------------------\n",
    "best_name = res_df.iloc[0][\"model\"]\n",
    "print(f\"\\nBest model selected: {best_name}\")\n",
    "\n",
    "if best_name == \"XGBoost\":\n",
    "    sw_full = per_sample_weights(y, class_weights)\n",
    "    # need integer labels for fit, but weights from string y are fine\n",
    "    xgb.fit(X, y_int, sample_weight=per_sample_weights(y, class_weights))\n",
    "    best_model = xgb\n",
    "    out_file = \"water_quality_xgb_3class.pkl\"\n",
    "else:\n",
    "    logreg_pipe.fit(X, y)\n",
    "    best_model = logreg_pipe\n",
    "    out_file = \"water_quality_logreg_3class.pkl\"\n",
    "\n",
    "joblib.dump(best_model, out_file)\n",
    "print(f\"✅ Saved best 3-class model as: {out_file}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Sanity check\n",
    "# ---------------------------\n",
    "if best_name == \"XGBoost\":\n",
    "    y_pred_full = le.inverse_transform(best_model.predict(X))\n",
    "else:\n",
    "    y_pred_full = best_model.predict(X)\n",
    "\n",
    "print(\"\\nFull-data classification report (sanity check):\")\n",
    "print(classification_report(y, y_pred_full, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618b744-b4b3-4eb2-afb6-bafb3261d27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
